{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjZ4u1D3YD7a"
      },
      "outputs": [],
      "source": [
        "# Imports \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "# Torch Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Image Imports \n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "from torchvision.models.vgg import vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlsrWD6lEn9d",
        "outputId": "7eba8a9c-e491-4b00-d9ec-5f24c3ce0e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File managing\n",
        "\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/CovidNetImages/train.txt\"\n",
        "TRAIN_IMAGES = \"/content/drive/MyDrive/CovidNetImages/train\"\n",
        "\n"
      ],
      "metadata": {
        "id": "TvzIuzOy1fiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myData = pd.read_csv(TRAIN_CSV,delimiter=\" \",header=None)\n"
      ],
      "metadata": {
        "id": "MD_233a_40FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heads = ['id','name','class','type']\n",
        "myData.columns = heads\n",
        "negatives = myData[myData['class']=='negative']\n",
        "positives = myData[myData['class'] == 'positive']"
      ],
      "metadata": {
        "id": "QUi_xBZp42RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalNegatives = negatives.iloc[3008:]\n",
        "finalPositives = positives.iloc[3008:]\n",
        "positiveImages = finalPositives['name'].tolist()\n",
        "negativeImages = finalNegatives['name'].tolist()\n",
        "trainImages = positiveImages+negativeImages\n",
        "random.shuffle(trainImages)"
      ],
      "metadata": {
        "id": "5q8xGRqu6wWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gw7W-FuYd8c",
        "outputId": "79cd9416-5ad5-43ba-d16e-a821b88e02ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7ff3f6b73a30>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4zpd12nYkAG"
      },
      "outputs": [],
      "source": [
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
        "\n",
        "\n",
        "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
        "    return crop_size - (crop_size % upscale_factor)\n",
        "\n",
        "\n",
        "def train_hr_transform(crop_size):\n",
        "    return Compose([\n",
        "        Resize((crop_size,crop_size)),\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def train_lr_transform(crop_size, upscale_factor):\n",
        "    return Compose([\n",
        "        # ToPILImage(),\n",
        "        Resize((crop_size // upscale_factor,crop_size // upscale_factor), interpolation=Image.NEAREST),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "def display_transform():\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize((400,400)),\n",
        "        CenterCrop((400,400)),\n",
        "        ToTensor()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvl2IkT1YnO4"
      },
      "outputs": [],
      "source": [
        "class TrainDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
        "        super(TrainDatasetFromFolder, self).__init__()\n",
        "        self.image_filenames = trainImages\n",
        "        # print(len(self.image_filenames))\n",
        "        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
        "        self.hr_transform = train_hr_transform(crop_size)\n",
        "        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imagePath = os.path.join(TRAIN_IMAGES,self.image_filenames[index])\n",
        "        im = Image.open(imagePath).convert('RGB')\n",
        "        hr_image = self.hr_transform(im)\n",
        "        lr_image = self.lr_transform(im)\n",
        "        # if index == 0:\n",
        "        #   print(\"HR: \",hr_image.size())\n",
        "        #   print(\"LR: \",lr_image.size())\n",
        "        # print(\"The self image is: \",self.image_filenames[index]\n",
        "        return lr_image, hr_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd0dY8s1YpeS"
      },
      "outputs": [],
      "source": [
        "UPSCALE_FACTOR = 2\n",
        "CROP_SIZE = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6Cvaju6Yreo"
      },
      "outputs": [],
      "source": [
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbz-2IbtYu2O",
        "outputId": "bb43b42c-03d3-4563-f255-b55aa6a7d2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_set = TrainDatasetFromFolder('/content/drive/MyDrive/CovidNetImages/train', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
        "# val_set = ValDatasetFromFolder('DIV2K_valid_HR', upscale_factor=UPSCALE_FACTOR)\n",
        "train_loader = DataLoader(dataset=train_set, num_workers=2, batch_size=8, shuffle=True)\n",
        "# val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClTPvoIPY51b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGXldFd1nVX3"
      },
      "outputs": [],
      "source": [
        "# class ConvBlock(nn.Module):\n",
        "#   def __init__(self,in_channels,out_channels):\n",
        "#     super().__init__()\n",
        "#     self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "#     self.activation = nn.PReLU()\n",
        "#     self.bacthNorm = nn.BatchNorm2d(out_channels)\n",
        "#     self.conv2 = nn.Conv2\n",
        "\n",
        "#   def forward(self,x:torch.Tensor):\n",
        "#     x = se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX_4himKZQ9m"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.prelu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        residual = self.bn2(residual)\n",
        "\n",
        "        return x + residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcNTaIZUZV4g"
      },
      "outputs": [],
      "source": [
        "class UpsampleBLock(nn.Module):\n",
        "    def __init__(self, in_channels, up_scale):\n",
        "        super(UpsampleBLock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.prelu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJjFHOq7Za9y"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        super().__init__()\n",
        "        upsample_block_num = int(math.log(scale_factor, 2))\n",
        "\n",
        "        self.firstConv = nn.Sequential(\n",
        "            nn.Conv2d(3,64,kernel_size=9,padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.residualBlocks = nn.ModuleList([\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "        ])\n",
        "        self.middleConv = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        ) \n",
        "        self.upscaleBlocks = nn.ModuleList([\n",
        "            UpsampleBLock(64, 2),\n",
        "            UpsampleBLock(64, 2),\n",
        "        ])\n",
        "\n",
        "        self.finalConv = nn.Conv2d(64,3,kernel_size=9,padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block1 = self.firstConv(x)\n",
        "        block2 = self.residualBlocks[0](block1)\n",
        "        block3 = self.residualBlocks[1](block2)\n",
        "        block4 = self.residualBlocks[2](block3)\n",
        "        block5 = self.residualBlocks[3](block4)\n",
        "        block6 = self.residualBlocks[4](block5)\n",
        "        block7 = self.middleConv(block6)\n",
        "        block8 = self.upscaleBlocks[0](block1 + block7)\n",
        "        block9 = self.upscaleBlocks[1](block8)\n",
        "        block10= self.finalConv(block9)\n",
        "        return (torch.tanh(block10) + 1) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzWaAeFRttcD"
      },
      "outputs": [],
      "source": [
        "class DiscConvBlocks(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels,out_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.batchNorm1 = nn.BatchNorm2d(out_channels)\n",
        "    self.activation1 =  nn.LeakyReLU(0.2)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1,stride=2)\n",
        "    self.batchNorm2 = nn.BatchNorm2d(out_channels)\n",
        "    self.activation2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchNorm1(x)\n",
        "    x = self.activation1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchNorm2(x)\n",
        "    x = self.activation2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjV7TIDBwwAG"
      },
      "outputs": [],
      "source": [
        "class finalConvBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.batchNorm1 =        nn.BatchNorm2d(in_channels)\n",
        "    self.activation1 =         nn.LeakyReLU(0.2)\n",
        "    self.pool =         nn.AdaptiveAvgPool2d(1)\n",
        "    self.conv2 =        nn.Conv2d(in_channels,out_channels, kernel_size=1)\n",
        "    self.activation2 =        nn.LeakyReLU(0.2)\n",
        "    self.conv3 =         nn.Conv2d(out_channels,3 , kernel_size=1)\n",
        "  \n",
        "  def forward(self,x:torch.Tensor):\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchNorm1(x)\n",
        "    x = self.activation1(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.activation2(x)\n",
        "    x = self.conv3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5UON38tZgdQ"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Discriminator, self).__init__()\n",
        "      # self.net = nn.Sequential(\n",
        "      #     nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "      #     nn.LeakyReLU(0.2), )\n",
        "      self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "      self.activation1 =     nn.LeakyReLU(0.2)\n",
        "      self.conv2 = nn.Conv2d(64,64,kernel_size=3,stride=2,padding=1)\n",
        "      self.batchNorm = nn.BatchNorm2d(64)\n",
        "      self.activation2 = nn.LeakyReLU(0.2)\n",
        "      \n",
        "      # These are for the final layers\n",
        "      self.pooling = nn.AdaptiveAvgPool2d(1)\n",
        "      self.flatten = nn.Conv2d(512,1024,kernel_size=1,)\n",
        "      self.activation3 = nn.LeakyReLU(0.2) \n",
        "      self.flatten2 = nn.Conv2d(1024,1,kernel_size=1)\n",
        "\n",
        "      self.convBlocks = nn.ModuleList([\n",
        "          DiscConvBlocks(64,128),\n",
        "          DiscConvBlocks(128,256),\n",
        "          DiscConvBlocks(256,512),\n",
        "      ])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.activation1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.batchNorm(x)\n",
        "      x = self.activation2(x)\n",
        "      x = self.convBlocks[0](x)\n",
        "      x = self.convBlocks[1](x)\n",
        "      x = self.convBlocks[2](x)\n",
        "      x = self.pooling(x)\n",
        "      x = self.flatten(x)\n",
        "      x = self.activation3(x)\n",
        "      x = self.flatten2(x)\n",
        "      batch_size = x.size(0)\n",
        "      return torch.sigmoid(x.view(batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsYmsy4vZhKg"
      },
      "outputs": [],
      "source": [
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        vgg = vgg16(pretrained=True)\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
        "        for param in loss_network.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.loss_network = loss_network\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        # Adversarial Loss\n",
        "        adversarial_loss = torch.mean(1 - out_labels)\n",
        "        # Perception Loss\n",
        "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
        "        # Image Loss\n",
        "        image_loss = self.mse_loss(out_images, target_images)\n",
        "        # TV Loss\n",
        "        tv_loss = self.tv_loss(out_images)\n",
        "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9umnAJdZoBS"
      },
      "outputs": [],
      "source": [
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):\n",
        "        return t.size()[1] * t.size()[2] * t.size()[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBgqUIIdZyRb",
        "outputId": "382e5687-c396-4a01-cdf2-75422452b00d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w2S3iavZ0aY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "d878bf23291e45f1bedd7ee1c9628cfd",
            "012bf0df36064aa5822569346801dfd4",
            "0ad20167e575486584c9681f05cea18d",
            "75df4abf0d2b4f52b97c9e45a88544d0",
            "58c0b0cd7ca043b4ba3c196d60987077",
            "18f2fcdca590490aa1f0ba48d2eb3ba3",
            "6b8563d693f5426394b774c0dd5dfbe9",
            "4843827e8a40466580ceaf1b1ff594ec",
            "50578478a42549269a0fbf8b3fcb9688",
            "fff851304eb14b9eae9bf2e2f1c4eae3",
            "4ebd099cbe154cadb3c97caed6471e15"
          ]
        },
        "outputId": "7d9d4100-dbee-4b66-da33-381036391d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d878bf23291e45f1bedd7ee1c9628cfd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "netG = Generator(UPSCALE_FACTOR)\n",
        "netD = Discriminator()\n",
        "generator_criterion = GeneratorLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D84qVbmxZ5WX"
      },
      "outputs": [],
      "source": [
        "# generator_criterion = generator_criterion.to('cpu')\n",
        "generator_criterion = generator_criterion.to(device)\n",
        "netG = netG.to(device)\n",
        "netD = netD.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q21RfSGcZ7X0"
      },
      "outputs": [],
      "source": [
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002)\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APt2NLY5Z9Xi"
      },
      "outputs": [],
      "source": [
        "results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqu423YtZ-AT"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POCl15bMibnZ"
      },
      "outputs": [],
      "source": [
        "# t = next(iter(train_loader))\n",
        "# t[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjvJ58hiqUQA"
      },
      "outputs": [],
      "source": [
        "# from torchsummary import summary\n",
        "# summary(netG,(3,22,22))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIdUAXDiu-06"
      },
      "outputs": [],
      "source": [
        "# summary(netD,(3,88,88))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generatorLoss = []\n",
        "discriminatorLoss = []\n",
        "import sys"
      ],
      "metadata": {
        "id": "ZbW2w1oo_0Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rsGU-GenLZQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPZxiR-faJRj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "2531a895-d1e3-4b55-9c85-0bd72a54b55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2997 [00:32<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d308d568503c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# fake_img = fake_img.to('cpu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# real_img =  real_img.to('cpu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-733e6b4b7be4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, out_labels, out_images, target_images)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0madversarial_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Perception Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mperception_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Image Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 14.76 GiB total capacity; 13.53 GiB already allocated; 39.88 MiB free; 13.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    train_bar = tqdm(train_loader)\n",
        "    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
        "\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "    gL,dL = 0,0\n",
        "    for data, target in train_bar:\n",
        "        # print(data.size())\n",
        "        # print(target.size())\n",
        "        g_update_first = True\n",
        "        batch_size = data.size(0)\n",
        "        running_results['batch_sizes'] += batch_size\n",
        "        \n",
        "        real_img = Variable(target)\n",
        "        z = Variable(data)\n",
        "        if torch.cuda.is_available():\n",
        "            real_img = real_img.cuda()\n",
        "            z = z.cuda()\n",
        "        \n",
        "        fake_img = netG(z) # z-> 22 *22 fakeImg -> 88*88\n",
        "        netD.zero_grad()\n",
        "        real_out = netD(real_img) # RealImg ->org 88*88\n",
        "        real_out = real_out.mean()\n",
        "        fake_out = netD(fake_img).mean() # gen 88*88\n",
        "        d_loss = 1 - real_out + fake_out\n",
        "        d_loss.backward(retain_graph=True)\n",
        "        optimizerD.step()\n",
        "\n",
        "        fake_img = netG(z)\n",
        "        fake_out = netD(fake_img).mean()\n",
        "        netG.zero_grad()\n",
        "        # fake_out = fake_out.to('cpu')\n",
        "        # fake_img = fake_img.to('cpu')\n",
        "        # real_img =  real_img.to('cpu')\n",
        "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
        "        g_loss = g_loss.to(device)\n",
        "        g_loss.backward()\n",
        "\n",
        "        # fake_out = fake_out.to(device)\n",
        "        # fake_img = fake_img.to(device)\n",
        "\n",
        "        # fake_img = netG(z)\n",
        "        # fake_out = netD(fake_img).mean()\n",
        "\n",
        "        optimizerG.step()\n",
        "\n",
        "        running_results['g_loss'] += g_loss.item() * batch_size\n",
        "        running_results['d_loss'] += d_loss.item() * batch_size\n",
        "        running_results['d_score'] += real_out.item() * batch_size\n",
        "        running_results['g_score'] += fake_out.item() * batch_size\n",
        "        gL += running_results['g_loss']\n",
        "        dL += running_results['d_loss']\n",
        "        train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
        "            epoch, N_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "            running_results['g_loss'] / running_results['batch_sizes'],\n",
        "            running_results['d_score'] / running_results['batch_sizes'],\n",
        "            running_results['g_score'] / running_results['batch_sizes']))\n",
        "    torch.cuda.empty_cache()\n",
        "    netG.eval()\n",
        "    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
        "    torch.save(netG,\"/content/drive/MyDrive/CovidNetImages/SavedSR/generator\"+epoch+\".pt\")\n",
        "    generatorLoss.append(gL)\n",
        "    discriminatorLoss.append(dL)\n",
        "    if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbw6acFcKrDe"
      },
      "outputs": [],
      "source": [
        "torch.save(netG,\"/content/drive/MyDrive/CovidNetImages/SavedSR/generatorV1.pt\")\n",
        "torch.save(netD,\"/content/drive/MyDrive/CovidNetImages/SavedSR/discV1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newModel = Generator(UPSCALE_FACTOR)\n",
        "newModel = torch.load(\"/content/drive/MyDrive/CovidNetImages/SavedSR/generatorV1.pt\")\n",
        "newModel =newModel.to('cpu')"
      ],
      "metadata": {
        "id": "J9a6hBQP57co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bar = tqdm(train_loader)\n",
        "running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}"
      ],
      "metadata": {
        "id": "E1fzPhMS-GMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "tVQkEpRFDQQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "UHS6pNyoFoam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data,path in train_loader:\n",
        "  z = Variable(data)\n",
        "  fake_img = newModel(z)\n",
        "  for i in range(16):\n",
        "    save_image(fake_img[i],\"/content/drive/MyDrive/CovidNetImages/SR_Out/\"+path[i])"
      ],
      "metadata": {
        "id": "KjipwoSA-fxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d878bf23291e45f1bedd7ee1c9628cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_012bf0df36064aa5822569346801dfd4",
              "IPY_MODEL_0ad20167e575486584c9681f05cea18d",
              "IPY_MODEL_75df4abf0d2b4f52b97c9e45a88544d0"
            ],
            "layout": "IPY_MODEL_58c0b0cd7ca043b4ba3c196d60987077"
          }
        },
        "012bf0df36064aa5822569346801dfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f2fcdca590490aa1f0ba48d2eb3ba3",
            "placeholder": "​",
            "style": "IPY_MODEL_6b8563d693f5426394b774c0dd5dfbe9",
            "value": "100%"
          }
        },
        "0ad20167e575486584c9681f05cea18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4843827e8a40466580ceaf1b1ff594ec",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50578478a42549269a0fbf8b3fcb9688",
            "value": 553433881
          }
        },
        "75df4abf0d2b4f52b97c9e45a88544d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff851304eb14b9eae9bf2e2f1c4eae3",
            "placeholder": "​",
            "style": "IPY_MODEL_4ebd099cbe154cadb3c97caed6471e15",
            "value": " 528M/528M [00:04&lt;00:00, 169MB/s]"
          }
        },
        "58c0b0cd7ca043b4ba3c196d60987077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f2fcdca590490aa1f0ba48d2eb3ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b8563d693f5426394b774c0dd5dfbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4843827e8a40466580ceaf1b1ff594ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50578478a42549269a0fbf8b3fcb9688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fff851304eb14b9eae9bf2e2f1c4eae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ebd099cbe154cadb3c97caed6471e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}